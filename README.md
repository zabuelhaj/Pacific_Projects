# Pacific_Projects
A culmination of my work (Independent and with teams) at University of the Pacific; from Undergraduate to Graduate.

### [Microcontrollers](https://github.com/zabuelhaj/Pacific_Projects/tree/master/MP3_Full_Code) ###  
This is an MP3 player that I worked on throughout my Fall 2017 semester. It was developed in C with one portion developed in ARM/Thumb-2 Assembly; the entire project was developed with direct-register manipulation.

### [Embedded Systems](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Embedded_Systems) ###  
These are a handful of projects I completed in my Embedded Systems course during Spring 2018. All of the projects were completed in C, with a couple being conceptual (thus they were not developed on a microcontroller).  

[Real-Time Shortest Job First](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Embedded_Systems/Final_Project)  
This was my team's final project for the course. It is a real-time scheduler that takes push-button inputs from a user to add processes to a queue. When a process is added, the queue adjusts to prioritize processes. The final project was completed using the TivaWare API library.  
[Traffic Scheduler](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Embedded_Systems/Traffic_Scheduling)  
This project was to implement scheduling for traffic lights at an intersection. It uses push-button input to add an ambulance, in which case the lights will change to control traffic and give the ambulance's lane a green light. It was completed using direct-register manipulation.  
[Scheduling Concepts](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Embedded_Systems/Concepts_Scheduling)  
These two projects were completed to test concepts learned in class. One is the classic Hungry Philosophers problem; the other is a simple Queue implementation.

### [Autonomous Robotics](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Autonomous_Robotics) ###  
Because I completed both the Bachelor's and Master's programs at Pacific, I had the opportunity to enroll in both graduate and undergraduate robotics courses. These both focused on autonomous robotic systems, with the "brains" being a Raspberry Pi and TM4C Microcontroller.  

[Basic Implementations](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Autonomous_Robotics/Undergraduate_Projects)  
The source code found here is from the undergraduate course. I worked with a teammate throughout the semester, using Python on the Raspberry Pi and C on the microcontroller. The microcontroller code was developed with the TivaWare API.  

There are four directories here: [Base_Code](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Autonomous_Robotics/Undergraduate_Projects/Base_Code) is, well... The base code that was developed throughout the semester. The other three directories are additional files or modifications made to the base code to complete different tasks. An example being [Potential_Fields](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Autonomous_Robotics/Undergraduate_Projects/Potential_Fields), which contains a Matlab simulation I made as well as the Python code developed to implement this algorithm on the robot. [Grid_Map](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Autonomous_Robotics/Undergraduate_Projects/Grid_Map) is a basic implementation of a single-heuristic grid map. [Final_Competition](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Autonomous_Robotics/Undergraduate_Projects/Final_Competition) is the modified code used to traverse the robot through a maze; our final was a competition between each group to traverse to the end of a maze.  

[Advanced Implementations](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Autonomous_Robotics/Graduate_Projects)  
These are some examples of the more advanced, or higher-level, concepts I implemented in graduate robotics. We used the same robot as the undergraduate course, but focused on the Raspberry Pi. This was because the Raspberry Pi was better suited to handle more complex algorithms, such as computer vision integration and mapping + localization. The microcontroller was still present; it just handled the motion control and low-level sensing. The Raspberry Pi code was implemented in Python, using various modules, such as OpenCV and NumPy.  

There are three directories here, I will try to unpack them the best I can:  
[Tracking_Coverage](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Autonomous_Robotics/Graduate_Projects/Tracking_Coverage) contains a tracking algorithm, the scripts used to prepare the tracking approach, and a FastSLAM module I wrote. For the Tracking approach, my teammate and I decided to go with a Haar Cascade Classifier. There are a couple reasons for this:  
1. Deep Learning would not require any complicated image processing algorithms to be introduced to the code. We could also use a more feature-rich object to track, in which we used a cartoon character.  
2. Since we were using OpenCV, we had access to some Machine Learning modules. Haar Cascade was one of the available options, and we went with it since we could just take some pictures of our object, pre-process them, and wait for the learning process to complete.  

Now to discuss the files: [Haar_Cascade_Scripts](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Autonomous_Robotics/Graduate_Projects/Tracking_Coverage/Haar_Cascade_Scripts) holds several scripts my teammate and I wrote and utilized to gather positive and negative images for our Haar Cascade Classifier. Rather than take several hundred photos by hand and spend time cropping them, or etc., we utilized these scripts instead. We have a script that splits videos by frames, producing images for us. Another script goes through these images and crops-out our tracking object through an edge-detection algorithm provided by OpenCV. These pre-processed, positive images then went through our classifier with the negative images. Negative images were randomly gathered by a script that pulled them off flckr. [Tracking_Only.py](https://github.com/zabuelhaj/Pacific_Projects/blob/master/Autonomous_Robotics/Graduate_Projects/Tracking_Coverage/Tracking_Only.py) used the knowledge files produced by the Haar Cascade Classifier to have the robot follow our tracking object.  
Unrelated to tracking is [fastSLAM.py](https://github.com/zabuelhaj/Pacific_Projects/blob/master/Autonomous_Robotics/Graduate_Projects/Tracking_Coverage/fastSLAM.py); this is a Python module I implemented myself to be used in our tracking algorithm. I decided that I wanted to tackle a SLAM algorithm for many reasons: fame, glory, the algorithm's efficiency, and because it's a popular algorithm used by robotics companies (TL;DR fame and glory). I wrote this code as a module only, so it could be used regardless of the accompanying tracking algorithm. Essentially, I integrated this module in code very similar to Tracking_Only.py. However, the modified tracking code was not very pretty and not a good representation of my work, so I just have fastSLAM.py here on its own.  

[Multi_Agent](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Autonomous_Robotics/Graduate_Projects/Multi_Agent) has mine and my teammate's code for a Multi-Agent robot system. This was used for a project that involved every group's robot; the objective was a game of Simon Says. Each robot had to communicate with eachother: one robot would start as Simon, give a command, then pick another robot to be Simon. The other robots had to perform the correct command and wait to see who was the next Simon. If a robot did not respond when selected as Simon or performed the wrong action, they were dropped from the match. Each robot in the Multi-Agent system used a ZigBee/XBee radio for wireless communication. Every group collaborated on a official communication protocol for the robots to use.  

[Final_Competition](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Autonomous_Robotics/Graduate_Projects/Final_Competition) contains the source code I wrote and contributed to for the final project, which was a tournament of Capture-the-Flag. The class was divided into four teams, each one having four robots. The goal was for each group to develop a Multi-Agent system to complete a game of capture the flag. My team had two offensive robots and two defensive robots. Our approach was a decentralized system, allowing each robot to work independently and communicate as-needed. I wrote the code for [commModule.py](https://github.com/zabuelhaj/Pacific_Projects/blob/master/Autonomous_Robotics/Graduate_Projects/Final_Competition/commModule.py) and [grabControl.py](https://github.com/zabuelhaj/Pacific_Projects/blob/master/Autonomous_Robotics/Graduate_Projects/Final_Competition/grabControl.py); these were modules to utilize a Multi-Agent communication system and to control grippers on the offensive robots, respectively. I additionally contributed to [mainOffensive.py](https://github.com/zabuelhaj/Pacific_Projects/blob/master/Autonomous_Robotics/Graduate_Projects/Final_Competition/mainOffense.py), which used my aforementioned code and contained the logic to search for the opposing team's flag.  

After I completed both robotics courses, I went on to do academic research for autonomous robotics using UAVs. You can read a bit about the research [here](https://github.com/zabuelhaj/Academic_Research).  

### High-Performance Computing ###  
I will soon be updating my GitHub with OpenMP, MPI, and CUDA projects I completed at University of the Pacific.  

### Machine Learning ###  
I will soon be updating my GitHub with various Machine Learning projects, such as NNs, CNNs, Reinforcement Learning, and more.  

### RAMA - The Robotic Arm Medical Assistant ###  
I will soon be updating my GitHub with my Senior Capstone Project code. Here's a link to a video Pacific shared of my team's work: [Click_Here](https://www.instagram.com/p/BiAo9EGleas/?igshid=pxp0r4pb9vht).  

### [FPGA Digital Design](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Digital_Design) ###  
These are all of my Advanced Digital Design projects from Pacific. Each directory is an individual project completed, where each project has a Synthesis Design and Simulation portion (sans the Vending Machine and Device Under Test projects). All the work was simulated in ModelSim and Synthesized on an Altera Cyclone IV FPGA dev board. The Synthesis Designs were programmed in SystemVerilog. Note: some files may have code commented out; this is because labs sometimes required comparing different approaches. Additionally, numerical input was done by toggle-switches on the dev board.  

[Vending Machine](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Digital_Design/Vending_Machine)  
The "Vending Machine" project is a simple LED controller that takes user input through the Cyclone IV input switches. It acts like a vending machine in the sense that you make a selection and the LEDs output in corresponging patterns: chasing left, flashing, all-on, and default (all-off). The project revolved around the comparison of a Mealy vs Moore Finite State Machine (FSM), of which both are included in the LED_FSM.sv module. Vending_Machine.sv is the top-level module. A clock divider was used to make the LED patterns visible, and was provided as boiler-plate code.  
[Device Under Test](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Digital_Design/Device_Under_Test)  
This is the only project that doesn't include Synthesis Design code, as the goal was to develop a ModelSim TestBench in SystemVerilog to discover information about an unknown Device Under Test (DUT). The TestBench file, lab2testbench.sv, utilizes a testCase() task to input 16 4-bit binary number combinations and output which combination was correct.  
[Combination Lock](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Digital_Design/Combination_Lock)  
This project is a combinational lock controller that accepts user input from three push-buttons available on the Cyclone IV dev board (Active-low A, B, and C). Because push-buttons are asynchronous, the state of the input is not definitive; thus the inclusion of the synchronizer circuit sync.sv. A Moore FSM is used, included in comb_FSM.sv, and the top-level module is Comb_Lock.sv.  
[Adder-Subtractor Unit](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Digital_Design/Sum_Difference)  
The goal of this project was to develop and verify a Two's-Compliment, 6-bit Adder/Subtractor unit on the Cyclone IV FPGA. It uses switches for input and a push-button to perform designated operations. It outputs numerical values onto two 7-segment displays as well as overflow and sign on LEDs. Simple control logic is used, which decides whether to output either the sum or difference, which are provided by the ADD.sv and SUB.sv modules respectively. The sum/difference is then passed through a sign-to-magnitude module, sign_mag.sv, that returns the sign and magnitude of the result. This then gets "spliced" through the splicer.sv module for the 7-segment display output; the sign (+/-) and overflow are then output on LEDs. The 7-segment display and associated LEDs will work while inputing A and B (where A + B = C or A - B = C). Because this project uses push-button input, a synchronizer is used. The top-level module is Add_Sub_Top.sv.  
[Handshaking](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Digital_Design/Handshaking)  
The scope of this project was to design and verify a simple asynchronous, bidirectional communication protocol with handshaking between two Cyclone IV FPGAs. deviceA.sv is the Moore FSM used to handle whether or not the FPGA is in write mode, read mode, done, and idle. A clock divider, clockdiv.sv, is used to slow-down the process for implementation. Because a push-botton puts the FPGA in write mode, a synchronizer, sync.sv, was included. A reset module, resetSync.sv, was included as well, to return the FSM to a known state during implementation. Handshaking.sv is the top-level module. It is also important to note that the FPGAs really only ping eachother (e.g. device A goes into write mode, handshakes with device B, and device B flashes an LED indicating some form of communication took place).  
[First In First Out Implementation](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Digital_Design/FIFO)  
This is a FIFO implementation; it can store 8 15-bit numbers and uses a toggle switch to switch between read and write modes. deviceFSM.sv is the Moore FSM used for the control logic, sync.sv to synchronize push-button input to commit a read or write, and memModule.sv is the memory module that writes or reads data in memory (as well as initialize it on start-up). FIFO.sv is the top-level module. The 7-segment display will either output the write numbers committed or the data read in memory. LED output shows whether the FIFO is empty or full, with that logic being handled inside the deviceFSM.sv module.  
[Basic Arithmetic Logic Unit](https://github.com/zabuelhaj/Pacific_Projects/tree/master/Digital_Design/Basic_ALU)  
The Basic ALU is somewhat an extension of the Adder-Subtractor Unit that was previously mentioned. It has the same ADD.sv and SUB.sv modules, with additional less-than, greather-than, equals, and equals-zero modules. Most of the modules used in this ALU are re-used from the aforementioned projects, with basically one new module (besides the comparators): Operation.sv. The operation module takes in two input numbers and outputs a value (sum, difference, or comparative boolean) depending on the selected operation and acts as a Moore FSM. The ALU_FSM.sv module is very similar to the FIFO.sv module, as it is primarily used to control the memory module.
